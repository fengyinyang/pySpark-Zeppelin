{"paragraphs":[{"text":"import org.apache.spark._\nimport org.apache.spark.graphx._\n// To make some of the examples work we will also need RDD\nimport org.apache.spark.rdd.RDD","dateUpdated":"Feb 18, 2016 11:00:53 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1455803782020_-887171808","id":"20160218-075622_1983929974","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark._\nimport org.apache.spark.graphx._\nimport org.apache.spark.rdd.RDD\n"},"dateCreated":"Feb 18, 2016 7:56:22 AM","dateStarted":"Feb 18, 2016 11:00:53 PM","dateFinished":"Feb 18, 2016 11:00:56 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2394"},{"text":"class VertexProperty()\ncase class UserProperty(val name: String) extends VertexProperty\ncase class ProductProperty(val name: String, val price: Double) extends VertexProperty\n// The graph might then have the type:\nvar graph: Graph[VertexProperty, String] = null","dateUpdated":"Feb 18, 2016 11:00:53 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1455803788444_-824442413","id":"20160218-075628_859343270","result":{"code":"SUCCESS","type":"TEXT","msg":"defined class VertexProperty\ndefined class UserProperty\ndefined class ProductProperty\ngraph: org.apache.spark.graphx.Graph[VertexProperty,String] = null\n"},"dateCreated":"Feb 18, 2016 7:56:28 AM","dateStarted":"Feb 18, 2016 11:00:54 PM","dateFinished":"Feb 18, 2016 11:00:59 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2395"},{"text":"// Assume the SparkContext has already been constructed\n//val sc: SparkContext\nsc.version\n// Create an RDD for the vertices\nval users: RDD[(VertexId, (String, String))] =\n  sc.parallelize(Array((3L, (\"rxin\", \"student\")), (7L, (\"jgonzal\", \"postdoc\")),\n                       (5L, (\"franklin\", \"prof\")), (2L, (\"istoica\", \"prof\"))))\n// Create an RDD for edges\nval relationships: RDD[Edge[String]] =\n  sc.parallelize(Array(Edge(3L, 7L, \"collab\"),    Edge(5L, 3L, \"advisor\"),\n                       Edge(2L, 5L, \"colleague\"), Edge(5L, 7L, \"pi\")))\n// Define a default user in case there are relationship with missing user\nval defaultUser = (\"John Doe\", \"Missing\")\n// Build the initial Graph\nval graph = Graph(users, relationships, defaultUser)","dateUpdated":"Feb 18, 2016 11:00:53 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1455804639903_604938621","id":"20160218-081039_1064619086","result":{"code":"SUCCESS","type":"TEXT","msg":"res6: String = 1.6.0\nusers: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, (String, String))] = ParallelCollectionRDD[16] at parallelize at <console>:38\nrelationships: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[String]] = ParallelCollectionRDD[17] at parallelize at <console>:38\ndefaultUser: (String, String) = (John Doe,Missing)\ngraph: org.apache.spark.graphx.Graph[(String, String),String] = org.apache.spark.graphx.impl.GraphImpl@4a21c541\n"},"dateCreated":"Feb 18, 2016 8:10:39 AM","dateStarted":"Feb 18, 2016 11:00:57 PM","dateFinished":"Feb 18, 2016 11:01:02 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2396"},{"text":"//val graph: Graph[(String, String), String] // Constructed from above\n// Count all users which are postdocs\ngraph.vertices.filter { case (id, (name, pos)) => pos == \"postdoc\" }.count\n// Count all the edges where src > dst\ngraph.edges.filter(e => e.srcId > e.dstId).count","dateUpdated":"Feb 18, 2016 11:00:54 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1455804788664_509134501","id":"20160218-081308_1817447143","result":{"code":"SUCCESS","type":"TEXT","msg":"res14: Long = 1\nres16: Long = 1\n"},"dateCreated":"Feb 18, 2016 8:13:08 AM","dateStarted":"Feb 18, 2016 11:00:59 PM","dateFinished":"Feb 18, 2016 11:01:03 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2397"},{"text":"graph.edges.filter { case Edge(src, dst, prop) => src > dst }.count","dateUpdated":"Feb 18, 2016 11:00:54 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1455805549709_1233106572","id":"20160218-082549_384754406","result":{"code":"SUCCESS","type":"TEXT","msg":"res18: Long = 1\n"},"dateCreated":"Feb 18, 2016 8:25:49 AM","dateStarted":"Feb 18, 2016 11:01:02 PM","dateFinished":"Feb 18, 2016 11:01:04 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2398"},{"text":"//val graph: Graph[(String, String), String] // Constructed from above\n// Use the triplets view to create an RDD of facts.\nval facts: RDD[String] =\n  graph.triplets.map(triplet =>\n    triplet.srcAttr._1 + \" is the \" + triplet.attr + \" of \" + triplet.dstAttr._1)\nfacts.collect.foreach(println(_))","dateUpdated":"Feb 18, 2016 11:00:54 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1455805932565_371822351","id":"20160218-083212_1070989601","result":{"code":"SUCCESS","type":"TEXT","msg":"facts: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[40] at map at <console>:47\nrxin is the collab of jgonzal\nfranklin is the advisor of rxin\nistoica is the colleague of franklin\nfranklin is the pi of jgonzal\n"},"dateCreated":"Feb 18, 2016 8:32:12 AM","dateStarted":"Feb 18, 2016 11:01:03 PM","dateFinished":"Feb 18, 2016 11:01:05 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2399"},{"text":"%md\n## Triangle Counting\nA vertex is part of a triangle when it has two adjacent vertices with an edge between them. GraphX implements a triangle counting algorithm in the TriangleCount object that determines the number of triangles passing through each vertex, providing a measure of clustering. We compute the triangle count of the social network dataset from the PageRank section. Note that TriangleCount requires the edges to be in canonical orientation (srcId < dstId) and the graph to be partitioned using Graph.partitionBy.","dateUpdated":"Feb 18, 2016 11:10:07 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1455858577517_-336290393","id":"20160218-230937_319257810","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Triangle Counting</h2>\n<p>A vertex is part of a triangle when it has two adjacent vertices with an edge between them. GraphX implements a triangle counting algorithm in the TriangleCount object that determines the number of triangles passing through each vertex, providing a measure of clustering. We compute the triangle count of the social network dataset from the PageRank section. Note that TriangleCount requires the edges to be in canonical orientation (srcId &lt; dstId) and the graph to be partitioned using Graph.partitionBy.</p>\n"},"dateCreated":"Feb 18, 2016 11:09:37 PM","dateStarted":"Feb 18, 2016 11:09:54 PM","dateFinished":"Feb 18, 2016 11:09:55 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2400"},{"text":"// Triangle Counting\n// Load the edges in canonical order and partition the graph for triangle count\nval graph = GraphLoader.edgeListFile(sc, \"followers.txt\", true).partitionBy(PartitionStrategy.RandomVertexCut)\n// Find the triangle count for each vertex\nval triCounts = graph.triangleCount().vertices\n// Join the triangle counts with the usernames\nval users = sc.textFile(\"users.txt\").map { line =>\n  val fields = line.split(\",\")\n  (fields(0).toLong, fields(1))\n}\nval triCountByUsername = users.join(triCounts).map { case (id, (username, tc)) =>\n  (username, tc)\n}\n// Print the result\nprintln(triCountByUsername.collect().mkString(\"\\n\"))","dateUpdated":"Feb 18, 2016 11:13:32 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1455805953856_-188356724","id":"20160218-083233_857043810","result":{"code":"SUCCESS","type":"TEXT","msg":"graph: org.apache.spark.graphx.Graph[Int,Int] = org.apache.spark.graphx.impl.GraphImpl@8a9e531\ntriCounts: org.apache.spark.graphx.VertexRDD[Int] = VertexRDDImpl[318] at RDD at VertexRDD.scala:57\nusers: org.apache.spark.rdd.RDD[(Long, String)] = MapPartitionsRDD[323] at map at <console>:37\ntriCountByUsername: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[327] at map at <console>:42\n(justinbieber,0)\n(matei_zaharia,1)\n(ladygaga,0)\n(BarackObama,0)\n(jeresig,1)\n(odersky,1)\n"},"dateCreated":"Feb 18, 2016 8:32:33 AM","dateStarted":"Feb 18, 2016 11:13:32 PM","dateFinished":"Feb 18, 2016 11:13:33 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2401"},{"text":"// Connect to the Spark cluster\n//val sc = new SparkContext(\"spark://master.amplab.org\", \"research\")\n\n// Load my user data and parse into tuples of user id and attribute list\nval users = (sc.textFile(\"users.txt\")\n  .map(line => line.split(\",\")).map( parts => (parts.head.toLong, parts.tail) ))\n\n// Parse the edge data which is already in userId -> userId format\nval followerGraph = GraphLoader.edgeListFile(sc, \"followers.txt\")\n\n// Attach the user attributes\nval graph = followerGraph.outerJoinVertices(users) {\n  case (uid, deg, Some(attrList)) => attrList\n  // Some users may not have attributes so we set them as empty\n  case (uid, deg, None) => Array.empty[String]\n}\n\n// Restrict the graph to users with usernames and names\nval subgraph = graph.subgraph(vpred = (vid, attr) => attr.size == 2)\n\n// Compute the PageRank\nval pagerankGraph = subgraph.pageRank(0.001)\n\n// Get the attributes of the top pagerank users\nval userInfoWithPageRank = subgraph.outerJoinVertices(pagerankGraph.vertices) {\n  case (uid, attrList, Some(pr)) => (pr, attrList.toList)\n  case (uid, attrList, None) => (0.0, attrList.toList)\n}\n\nprintln(userInfoWithPageRank.vertices.top(5)(Ordering.by(_._2._1)).mkString(\"\\n\"))","dateUpdated":"Feb 18, 2016 11:16:06 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1455858054277_-784088851","id":"20160218-230054_465166254","result":{"code":"SUCCESS","type":"TEXT","msg":"users: org.apache.spark.rdd.RDD[(Long, Array[String])] = MapPartitionsRDD[345] at map at <console>:41\nfollowerGraph: org.apache.spark.graphx.Graph[Int,Int] = org.apache.spark.graphx.impl.GraphImpl@75ffe9e2\ngraph: org.apache.spark.graphx.Graph[Array[String],Int] = org.apache.spark.graphx.impl.GraphImpl@111d5aa6\nsubgraph: org.apache.spark.graphx.Graph[Array[String],Int] = org.apache.spark.graphx.impl.GraphImpl@13d8d12d\npagerankGraph: org.apache.spark.graphx.Graph[Double,Double] = org.apache.spark.graphx.impl.GraphImpl@11fcae0\nuserInfoWithPageRank: org.apache.spark.graphx.Graph[(Double, List[String]),Int] = org.apache.spark.graphx.impl.GraphImpl@3d26881e\n(1,(1.453834747463902,List(BarackObama, Barack Obama)))\n(2,(1.3857595353443166,List(ladygaga, Goddess of Love)))\n(7,(1.2892158818481694,List(odersky, Martin Odersky)))\n(3,(0.9936187772892124,List(jeresig, John Resig)))\n(6,(0.697916749785472,List(matei_zaharia, Matei Zaharia)))\n"},"dateCreated":"Feb 18, 2016 11:00:54 PM","dateStarted":"Feb 18, 2016 11:16:06 PM","dateFinished":"Feb 18, 2016 11:16:20 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2402"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1455858910168_-675768618","id":"20160218-231510_2054296265","dateCreated":"Feb 18, 2016 11:15:10 PM","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:2403"}],"name":"GraphX_fun","id":"2BCW6AC96","angularObjects":{"2BCFJMDKK":[],"2BDS4QD2N":[],"2BANE2ZZJ":[],"2BBTRQU62":[],"2BB79Q58G":[],"2BAJ9HSGF":[],"2BE8VSB6H":[],"2BD9EQUSM":[],"2BDCUE528":[],"2BBSMES14":[],"2BCN9XJ4P":[],"2BCT5B8KA":[],"2BBUUR2BV":[],"2BD9UFHB9":[]},"config":{"looknfeel":"default"},"info":{}}